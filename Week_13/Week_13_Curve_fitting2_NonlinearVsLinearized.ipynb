{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting nonlinear and linear models\n",
    "\n",
    "In this notebook, we will be exploring the difference between fitting nonlinear and linearized models.\n",
    "\n",
    "Nonlinear model fitting (which we covered last week) tries to match a function over some number inputs (x, or x, y) with some number of parameters to a set of data. One example is a Gaussian function, $f(x) = e^{\\frac{-(x-\\mu)^2}{2\\sigma}}$ ). This approach is generally taken when a strong hypothesis exists about the nonlinear relationships between variables in a data set. For example, this is the approach taken to estimate the parameters of population receptive fields in early visual cortex, given a strong hypothesis that the pRFs can be modeled as circular Gaussian functions. Nonliner curve fitting estimates values for the parameters (i.e., the variable that are not the inputs) in the hypothesized function. In the Gaussian example given above, this would be one value for $\\mu$, and one value for $\\sigma$; or, for a 2D spatial pRF, it would be one value for $\\mu_x$, one value for $\\mu_y$, and one value for $\\sigma$. \n",
    "\n",
    "Nonlinear model fitting, as we will see, is tricky. For nonlinear functions (such as a Gaussian), there is no guarantee that the estimate of the parameters for a given fit provides the *best possible fit* to the data.\n",
    "\n",
    "In this notebook, we will generate some data from a parametric function (a Gaussian), fit it with several polynomial functions, and evaluate which gives the best fit (just like we did last week). Then we will further explore WHY the fits for some functions are unstable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Curve fitting optimization\n",
    "from scipy import optimize\n",
    "\n",
    "# For display of error surface in 3D plots\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: generate some data\n",
    "We want some data that has some real, underlying, parametric model. We will use the same 1D Gaussian model that we used last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a function to define a 1D Gaussian curve:\n",
    "def gauss(x, mu, sigma, amp=1):\n",
    "    \"\"\"Estimate value of Gaussian function for specific value or values of x\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : scalar or array-like\n",
    "        value or values at which to evaluate the Gaussian\n",
    "    mu : scalar\n",
    "        mean of Gaussian\n",
    "    sigma : scalar\n",
    "        std of Gaussian\n",
    "    amp : amplitude of Gaussian\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    valueof Gaussian at `x`\n",
    "    \"\"\"\n",
    "    return amp * np.e**(-(x-mu)**2 / (2*sigma))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we pick a `mu` and `sigma` value, and decide which X values to use. We define `y` as the output of the `gauss()` function (with the `x`, `mu`, and `sigma` values as parameters) plus some noise. Then we make a scatter plot of `x` vs `y`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 101\n",
    "# x = np.random.rand(n) * 6 - 3\n",
    "x = np.random.uniform(low=-3, high=3, size=(n,))\n",
    "mu = -0.3\n",
    "sigma = 1.5\n",
    "\n",
    "y_perfect = gauss(x, mu=mu, sigma=sigma, amp=1)\n",
    "noise = np.random.normal(loc=0, scale=0.2, size=y_perfect.shape)\n",
    "y = y_perfect + noise\n",
    "plt.scatter(x, y,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will fit two different functions to the data, a Gaussian and sine function. First, we will define the sine function, then we will do the fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sine function for fitting\n",
    "def sinx(x, freq, phase, amp=1):\n",
    "    return np.sin(x * 2 * np.pi * freq + phase) * amp\n",
    "\n",
    "# Fit gaussian function\n",
    "gauss_guess = [1, 3, 1] # values for mu, sigma, amp\n",
    "params_gauss, pcov = optimize.curve_fit(gauss, x, y, p0=gauss_guess)\n",
    "mu_est, sig_est, ampg_est = params_gauss\n",
    "\n",
    "# Fit sine function\n",
    "sin_guess1 = [1, 0, 1] # values for freq, phase, amp\n",
    "params_sin1, pcov = optimize.curve_fit(sinx, x, y, p0=sin_guess1)\n",
    "freq_est1, phase_est1, amps_est1 = params_sin1\n",
    "\n",
    "# Fit sine function with a different guess for starting parameters\n",
    "sin_guess2 = [0.3, .25* np.pi, 1] # values for freq, phase, amp\n",
    "params_sin2, pcov = optimize.curve_fit(sinx, x, y, p0=sin_guess2)\n",
    "freq_est2, phase_est2, amps_est2 = params_sin2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOPSIES, these are different!\n",
    "print(params_sin1)\n",
    "print(params_sin2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "# Compute predictions on evenly spaced x data\n",
    "xtest = np.linspace(-3, 3, 101)\n",
    "gtest = gauss(xtest, mu_est, sig_est, ampg_est)\n",
    "stest1 = sinx(xtest, freq_est1, phase_est1, amps_est1)\n",
    "stest2 = sinx(xtest, freq_est2, phase_est2, amps_est2)\n",
    "\n",
    "# Plot data & predictions\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "ax.plot(xtest, gtest, color='orange', label='Gauss fit')\n",
    "ax.plot(xtest, stest1, color='r', label='Sin fit - guess 1')\n",
    "ax.plot(xtest, stest2, color='m', label='Sin fit - guess 2')\n",
    "ax.legend(loc='best')\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second guess at the sine fit is much better than the first guess. The error for that function is lower, too! WHY?? Is this fitting algorithm good for nothing? \n",
    "\n",
    "Let's play a game. Let's see how the parameter estimates for the sinx function vary with different starting guesses. \n",
    "\n",
    "> Write a loop to collect parameter estimates for different guesses at the frequency of the sin function (keep phase constant for now). See how many different values for the freq_est and phase_est you get!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Do the same for the `gauss` function. Do different guesses for `mu` yield different parameter results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the error surface\n",
    "\n",
    "The reason that initial guesses can affect parameter estimates differently is that the fitting function is *iterative*. It assesses whether the error for a given stage is better or worse than at the last stage. As soon as the error starts getting worse, the algorithm decides that it's found an error minimum, and returns parameter estimates. This means that the algorithm sometimes gets stuck in *local minima* that are not the *global minimum* for error. The following plots will show you what that means:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gauss error surface\n",
    "> Write in comments above each of these lines saying what they do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nmu = 11\n",
    "nsig = 11\n",
    "\n",
    "mu_vals = np.linspace(-2, 2, nmu)\n",
    "\n",
    "sigma_vals = np.linspace(0.1, 4, nsig)\n",
    "\n",
    "error_surface_gauss = np.zeros((nmu, nsig))\n",
    "\n",
    "for imu, mu_ in enumerate(mu_vals):\n",
    "\n",
    "    for isig, sigma_ in enumerate(sigma_vals):\n",
    "        \n",
    "        g = gauss(x, mu_, sigma_, amp=1)\n",
    "        \n",
    "        resids = np.sum((y-g)**2)\n",
    "        \n",
    "        error_surface_gauss[imu, isig] = resids\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(error_surface_gauss, extent=[0.1, 4, -2, 2], aspect='auto')\n",
    "ax.set_xlabel('Sigma value')\n",
    "ax.set_ylabel('Mu value')\n",
    "cb = fig.colorbar(im)\n",
    "cb.set_label('Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This error surface is reasonably well behaved - there is one big dip in the middle of this range of guesses, which is at the value of the true parameters. No matter where you start, an iterative process will converge on that minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sin error surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter guesses\n",
    "nf = 11\n",
    "nph = 11\n",
    "freq_vals = np.linspace(0, 1.0, 11)\n",
    "phase_vals = np.linspace(0, np.pi*2, 11)\n",
    "# Initialize error surface\n",
    "error_surface_sin = np.zeros((nf, nph))\n",
    "for ifreq, freq_ in enumerate(freq_vals):\n",
    "    for iphase, phase_ in enumerate(phase_vals):\n",
    "        # Compute prediction\n",
    "        yhat = sinx(x, freq_, phase_, amp=1)\n",
    "        # Compute residuals\n",
    "        resids = np.sum((y-yhat)**2)\n",
    "        error_surface_sin[ifreq, iphase] = resids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show error surface, simpler\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(error_surface_sin, extent=[0, np.pi*2, 1.0, 0], aspect='auto')\n",
    "ax.set_xlabel('phase value')\n",
    "ax.set_ylabel('freq value')\n",
    "cb = fig.colorbar(im)\n",
    "cb.set_label('Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show error surface, fancier\n",
    "yg, xg = np.meshgrid(phase_vals, freq_vals)\n",
    "fig, ax = plt.subplots(figsize=(8,8), subplot_kw=dict(projection='3d', azim=45, elev=25))\n",
    "ax.plot_surface(xg, yg, error_surface_sin, cmap='viridis', shade=False)\n",
    "ax.set_xlabel('Parameter 1 (Frequency)')\n",
    "ax.set_ylabel('Parameter 2 (Phase)')\n",
    "ax.set_zlabel('Error');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sin error surface is NOT convex / smooth... Lots of local minima! This is why different guesses result in different parameter estimtes - the algorithm \"falls into\" one of the dips (local minima) in the in the error surface, and assumes those are the best parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linearized regression models\n",
    "These unstable error surfaces are a big reason that many researchers use linear regression models. Linear models are guararanteed to have convex error surfaces over their paramters, meaning you will ALWAYS get the best estimate you can. This can mean that your estimate of your parameters - your $\\beta$ weights - are TOO good, though - so you still have to guard against over-fitting to noise. \n",
    "\n",
    "Here, we will explore how to make a lineari*ized* model of the gaussian data used above. For a linearized model, you basically take a bunch of guesses at the potential nonlinear parameters, and then use each guess as a regressor in your design matrix. \n",
    "\n",
    "That means, in this case,we will guess at a bunch of gaussians with different values for mu and sigma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearIZED model\n",
    "# NOTE: sigma_vals and mu_vals are the same we used above to explore the error surface.\n",
    "gausses = []\n",
    "for sigma_ in sigma_vals:\n",
    "    for mu_ in mu_vals:\n",
    "        this_regressor = gauss(x, mu_, sigma_)\n",
    "        gausses.append(this_regressor)\n",
    "# Combine all guesses in a \n",
    "Xreg = np.vstack(gausses).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression function.\n",
    "# Note that with a = 0, this is THE SAME as OLS regression!\n",
    "def ridge(X, Y, a=0):\n",
    "    b = np.linalg.inv(X.T.dot(X) + a * np.eye(X.shape[1])).dot(X.T.dot(Y))\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate beta weights for each of the predictors above (one per assumed Gaussian function)\n",
    "B_est = ridge(Xreg, y, a=0) # Start with a = 0 for OLS regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What is the size of beta? \n",
    "\n",
    "> Make a histogram plot of `B_est`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test the predictions of the regression model, within-set and out-of-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Within set prediction: predict the y values above\n",
    "yhat_reg = Xreg.dot(B_est)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "ax.plot(x, yhat_reg, '.', color='orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm. That's rough. Change the alpha value above (to anyting > 0) and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out-of-set prediction (to evenly spaced test values)\n",
    "gausses_test = []\n",
    "for sigma_ in sigma_vals:\n",
    "    for mu_ in mu_vals:\n",
    "        this_regressor = gauss(xtest, mu_, sigma_)\n",
    "        gausses_test.append(this_regressor)\n",
    "# Combine all guesses in a single array\n",
    "Xtest = np.vstack(gausses_test).T\n",
    "# Make predictions of evenly spaced x data\n",
    "yhat_reg = Xtest.dot(B_est)\n",
    "# Plot the results!\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xtest, yhat_reg, color='orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the beta weights to see what we've done. Remember, each beta reflects an individual Guassian function with different parameters for mu and theta. So let's reshape B_est to be (mu_values x sigma_values)\n",
    "\n",
    "> Reshape B_est as suggested above, and display it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How could you reduce the number of $\\beta$ values in this example? \n",
    "\n",
    "\n",
    "> What might you do to make the OLS fit better? \n",
    "\n",
    "\n",
    "> What are the advantages of each approach (nonlinear vs linearized regression estimates of model parameters)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
